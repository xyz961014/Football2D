import os
import sys
import numpy as np
import torch
import torch.nn as nn
from torch import optim

import ipdb


from pathlib import Path
parent_dir = Path(__file__).absolute().parent.parent.parent
sys.path.append(os.path.abspath(parent_dir))

from rl.utils import ScaleParameterizedNormal
from rl.algorithms.actor_critic import ActorCritic

class A2C(ActorCritic):
    """
    (Synchronous) Advantage Actor-Critic agent class

    Args:
        n_features: The number of features of the input state.
        n_actions: The number of actions the agent can take.
        device: The device to run the computations on (running on a GPU might be quicker for larger Neural Nets,
                for this code CPU is totally fine).
        critic_lr: The learning rate for the critic network (should usually be larger than the actor_lr).
        actor_lr: The learning rate for the actor network.
        n_envs: The number of environments that run in parallel (on multiple CPUs) to collect experiences.
    """

    def __init__(
        self,
        model_name: str,
        n_features: int,
        n_actions: int,
        hidden_size: int,
        output_activation: str,
        device: torch.device,
        critic_lr: float=5e-3,
        actor_lr: float=1e-3,
        weight_decay: float=0.0,
        init_scale: float=1.0,
        n_envs=1,
        ent_coef=1.0,
        max_grad_norm=1.0,
        train_scale=True,
        normalize_factor=1.0,
        dropout=0.0
    ) -> None:
        """Initializes the actor and critic networks and their respective optimizers."""
        super().__init__(model_name, n_features, n_actions, hidden_size, output_activation,
                         device, init_scale, n_envs, normalize_factor, dropout)
        self.ent_coef = ent_coef
        self.max_grad_norm = max_grad_norm

        # define optimizers for actor and critic
        self.critic_params = list(self.critic.parameters())
        self.actor_params = list(self.actor.parameters())
        if train_scale and hasattr(self.dist, "parameters"):
            self.actor_params.extend(list(self.dist.parameters()))
        self.critic_optim = optim.RMSprop(self.critic_params, lr=critic_lr, weight_decay=weight_decay)
        self.actor_optim = optim.RMSprop(self.actor_params, lr=actor_lr, weight_decay=weight_decay)

    def get_losses(
        self,
        memory,
    ) -> tuple([torch.Tensor, torch.Tensor, torch.Tensor]):
        """
        Computes the loss of a minibatch (transitions collected in one sampling phase) for actor and critic
        using Generalized Advantage Estimation (GAE) to compute the advantages (https://arxiv.org/abs/1506.02438).

        Args:
            rewards: A tensor with the rewards for each time step in the episode, with shape [n_steps_per_update, n_envs].
            action_log_probs: A tensor with the log-probs of the actions taken at each time step in the episode, with shape [n_steps_per_update, n_envs].
            value_preds: A tensor with the state value predictions for each time step in the episode, with shape [n_steps_per_update, n_envs].
            masks: A tensor with the masks for each time step in the episode, with shape [n_steps_per_update, n_envs].
            gamma: The discount factor.
            lam: The GAE hyperparameter. (lam=1 corresponds to Monte-Carlo sampling with high variance and no bias,
                                          and lam=0 corresponds to normal TD-Learning that has a low variance but is biased
                                          because the estimates are generated by a Neural Net).
            device: The device to run the computations on (e.g. CPU or GPU).

        Returns:
            critic_loss: The critic loss for the minibatch.
            actor_loss: The actor loss for the minibatch.
        """
        #advantages = memory.advantages
        #action_log_probs = memory.action_log_probs
        #entropy = self.dist(memory.actions[-1]).entropy()

        states = memory.concat_states()
        state_values, action_log_probs, entropy = self.evaluate_actions(states, memory.actions)
        value_preds = state_values.squeeze()

        advantages = memory.returns - value_preds

        # calculate the loss of the minibatch for actor and critic
        critic_loss = advantages.pow(2).mean()

        # give a bonus for higher entropy to encourage exploration
        actor_loss = (
            -(advantages.detach() * action_log_probs).mean() - self.ent_coef * entropy
        )
        return (critic_loss, actor_loss, entropy)

    def update_parameters(self, memory):
        """
        Updates the parameters of the actor and critic networks.

        Args:
            critic_loss: The critic loss.
            actor_loss: The actor loss.
        """
        critic_loss, actor_loss, entropy = self.get_losses(memory)

        self.critic_optim.zero_grad()
        critic_loss.backward()
        nn.utils.clip_grad_norm_(self.critic_params, self.max_grad_norm)
        self.critic_optim.step()

        self.actor_optim.zero_grad()
        actor_loss.backward()
        nn.utils.clip_grad_norm_(self.actor_params, self.max_grad_norm)
        self.actor_optim.step()

        return (critic_loss, actor_loss, entropy)


